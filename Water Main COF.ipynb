{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consequence of Failure Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proximity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "import pandas as pd\n",
    "from arcgis.features import GeoAccessor, GeoSeriesAccessor\n",
    "import os\n",
    "from datetime import datetime\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the workspace\n",
    "arcpy.env.workspace = r\"C:\\Users\\ggarcia\\OneDrive - Abonmarche\\GIS Projects\\2023\\23-0304 Grand Haven CDSMI\\ArcGIS Pro\\GH Risk Analysis\\Round3Analysis.gdb\"\n",
    "arcpy.env.overwriteOutput = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# water main feature class to dataframe\n",
    "water_main_fc = \"Round3WaterMain\"\n",
    "water_main_df = pd.DataFrame.spatial.from_featureclass(water_main_fc)\n",
    "# water_main_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OBJECTID</th>\n",
       "      <th>FACILITYID</th>\n",
       "      <th>DIAMETER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   OBJECTID FACILITYID  DIAMETER\n",
       "0         1          1       8.0\n",
       "1         2          2       6.0\n",
       "2         3          3       6.0\n",
       "3         4          4       4.0\n",
       "4         5          5      12.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop all columns except OBJECTID, FACILITYID, and DIAMETER\n",
    "water_main_df = water_main_df[['OBJECTID', 'FACILITYID', 'DIAMETER']]\n",
    "water_main_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all rows with null values\n",
    "water_main_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate Near Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of feature classes\n",
    "feature_classes = [\"Buildings\", \"Major_Intersection\", \"Major_Road\", \"Minor_Intersection\", \"Minor_Road\", \"Railroad\", \"ROW\", \"WaterAreas\", \"WaterLines\"]\n",
    "# Get the current working directory\n",
    "dir_path = os.getcwd()\n",
    "# Dictionary to store the dataframes\n",
    "dfs = {}\n",
    "\n",
    "# Iterate over the feature classes\n",
    "for fc in feature_classes:\n",
    "    # Create the output path\n",
    "    output_path = os.path.join(dir_path, \"Results\", \"near_\" + fc + \".csv\")\n",
    "    # Near analysis\n",
    "    near_table = arcpy.GenerateNearTable_analysis(\n",
    "        in_features=water_main_fc,\n",
    "        near_features=fc,\n",
    "        out_table=output_path,\n",
    "        search_radius=\"10000 Feet\",\n",
    "        location=\"NO_LOCATION\",\n",
    "        angle=\"NO_ANGLE\",\n",
    "        closest=\"CLOSEST\",\n",
    "        closest_count=\"0\",\n",
    "        method=\"PLANAR\",\n",
    "        distance_unit=\"Feet\")\n",
    "\n",
    "    # Convert near table csv to dataframe\n",
    "    near_df = pd.read_csv(output_path)\n",
    "\n",
    "    # remove the .xml, .ini, and .csv.xml files the geoprocess also created\n",
    "    for file in os.listdir(os.path.join(dir_path, \"Results\")):\n",
    "        if file.endswith(\".xml\") or file.endswith(\".ini\"):\n",
    "            os.remove(os.path.join(dir_path, \"Results\", file))\n",
    "\n",
    "    # rename the column in the near_df from NEAR_DIST to the name of the feature class\n",
    "    near_df.rename(columns = {'NEAR_DIST': fc}, inplace = True)\n",
    "    # drop the columns OBJECTID and NEAR_FID\n",
    "    near_df = near_df.drop(columns=['NEAR_FID'])\n",
    "\n",
    "    # Add the dataframe to the dictionary\n",
    "    dfs[fc] = near_df\n",
    "# copy the water_main_df to a new dataframe\n",
    "Near_results_df = water_main_df.copy()\n",
    "\n",
    "# Merge the dataframes with the Near_results_df removing the IN_FID column each time\n",
    "for key, value in dfs.items():\n",
    "    Near_results_df = pd.merge(Near_results_df, value, left_on='OBJECTID', right_on='IN_FID', how='left')\n",
    "    Near_results_df = Near_results_df.drop(columns=['IN_FID'])\n",
    "\n",
    "Near_results_df.head()\n",
    "# save the Near_results_df to a csv file using dir_path\n",
    "Near_results_df.to_csv(os.path.join(dir_path, \"Results\", \"NearResults.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affected Customer Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affected Service Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define isolation zones and lateral lines feature classes\n",
    "\n",
    "isolation_zones_fc = \"IsoZone\"\n",
    "lateral_lines_fc = \"Round3WaterLats\"\n",
    "\n",
    "# filter the laterals to only domestic lines\n",
    "domestic_lats = arcpy.management.SelectLayerByAttribute(lateral_lines_fc, \"NEW_SELECTION\", \"LINETYPE <> 'Hydrant'\")\n",
    "# spatial join laterals to isolation zones one to many so there is a row for each lateral in the isolation zone\n",
    "zones_lats_join = \"in_memory\\zones_lats_join\"\n",
    "\n",
    "arcpy.analysis.SpatialJoin(\n",
    "    target_features=isolation_zones_fc,\n",
    "    join_features=domestic_lats,\n",
    "    out_feature_class=zones_lats_join,\n",
    "    join_operation=\"JOIN_ONE_TO_MANY\",\n",
    "    join_type=\"KEEP_COMMON\",\n",
    "    match_option=\"INTERSECT\",\n",
    ")\n",
    "#summarize the join results\n",
    "\n",
    "summary_output = os.path.join(dir_path, \"Results\", \"zone_lat_summary\" + \".csv\")\n",
    "arcpy.analysis.Statistics(\n",
    "    in_table=zones_lats_join,\n",
    "    out_table=summary_output,\n",
    "    statistics_fields=\"FACILITYID COUNT\",\n",
    "    case_field=\"zone\",\n",
    ")\n",
    "# remove the .xml, .ini, and .csv.xml files the geoprocess also created\n",
    "for file in os.listdir(os.path.join(dir_path, \"Results\")):\n",
    "    if file.endswith(\".xml\") or file.endswith(\".ini\"):\n",
    "        os.remove(os.path.join(dir_path, \"Results\", file))\n",
    "# convert the summary table to a dataframe\n",
    "summary_df = pd.read_csv(summary_output)\n",
    "summary_df.head()\n",
    "# add a spatial join to the water main feature class to get the isolation zones into the water mains\n",
    "main_iso_join = \"in_memory\\main_iso_join\"\n",
    "\n",
    "arcpy.analysis.SpatialJoin(\n",
    "    target_features=water_main_fc,\n",
    "    join_features=isolation_zones_fc,\n",
    "    out_feature_class=main_iso_join,\n",
    "    join_operation=\"JOIN_ONE_TO_ONE\",\n",
    "    join_type=\"KEEP_ALL\",\n",
    "    match_option=\"LARGEST_OVERLAP\"\n",
    ")\n",
    "# Convert the feature class to a dataframe\n",
    "mains_iso_df = pd.DataFrame.spatial.from_featureclass(main_iso_join)\n",
    "mains_iso_df.head()\n",
    "# drop all columns except OBJECTID, FACILITYID, DIAMETER, and zone\n",
    "mains_iso_df = mains_iso_df[['OBJECTID', 'FACILITYID', 'DIAMETER', 'zone']]\n",
    "mains_iso_df.head()\n",
    "#  use the summary df as a key to add a column to the mains_iso_df for affected laterals and fill it with the count of laterals in the isolation zone\n",
    "mains_iso_df['affected_lats'] = mains_iso_df['zone'].map(summary_df.set_index('zone')['FREQUENCY'])\n",
    "mains_iso_df.head()\n",
    "# save the mains_iso_df to a csv using the dir_path\n",
    "mains_iso_df.to_csv(os.path.join(dir_path, \"Results\", \"mains_affected_lats_by_zones.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affected Critical Customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Schools/Childcare Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='gpresult'><h2>Messages</h2><div id='messages'>Start Time: Monday, July 22, 2024 9:36:19 AM<br>Succeeded at Monday, July 22, 2024 9:36:19 AM (Elapsed Time: 0.20 seconds)<br></div><div id = 'default' /></div></div>"
      ],
      "text/plain": [
       "<Result 'in_memory\\\\parcels_school_child_join'>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spatial join merged school and childcare feature class to parcels\n",
    "parcels_fc = \"Parcels\"\n",
    "school_childcare = \"School_Childcare\"\n",
    "\n",
    "parcels_school_child_join = \"in_memory\\parcels_school_child_join\"\n",
    "\n",
    "arcpy.analysis.SpatialJoin(\n",
    "    target_features=parcels_fc,\n",
    "    join_features=school_childcare,\n",
    "    out_feature_class=parcels_school_child_join,\n",
    "    join_operation=\"JOIN_ONE_TO_ONE\",\n",
    "    join_type=\"Keep_Common\",\n",
    ")\n",
    "# identify lateral lines that intersect the spatial join of parcels and school/childcare using spatial join\n",
    "lats_School_Child_Join = \"in_memory\\Lats_School_Child_Join\"\n",
    "\n",
    "arcpy.analysis.SpatialJoin(\n",
    "    target_features=lateral_lines_fc,\n",
    "    join_features=parcels_school_child_join,\n",
    "    out_feature_class=lats_School_Child_Join,\n",
    "    join_operation=\"JOIN_ONE_TO_ONE\",\n",
    "    join_type=\"KEEP_COMMON\",\n",
    ")\n",
    "# spatial join the school lateral results to the rest of the laterals by intersect to identify both sides of the service that serve the school/childcare\n",
    "lats_School_Child_Join_All = \"in_memory\\Lats_School_Child_Join_All\"\n",
    "\n",
    "arcpy.analysis.SpatialJoin(\n",
    "    target_features=lateral_lines_fc,\n",
    "    join_features=lats_School_Child_Join,\n",
    "    out_feature_class=lats_School_Child_Join_All,\n",
    "    join_operation=\"JOIN_ONE_TO_ONE\",\n",
    "    join_type=\"KEEP_COMMON\",\n",
    "    match_option=\"INTERSECT\",\n",
    ")\n",
    "# dissolve the spatial join results by parcel id to create one line feature for each school/childcare service connection\n",
    "lats_school_child_dissolve = \"in_memory\\Lats_School_Child_Dissolve\"\n",
    "\n",
    "arcpy.management.Dissolve(\n",
    "    in_features=lats_School_Child_Join_All,\n",
    "    out_feature_class=lats_school_child_dissolve,\n",
    "    dissolve_field=\"FinalPIN\",\n",
    ")\n",
    "# spatial join the dissolved feature class to the water main feature class to identify the mains that serve the school/childcare\n",
    "Main_School_Child_Join = \"in_memory\\Main_School_Child_Join\"\n",
    "\n",
    "arcpy.analysis.SpatialJoin(\n",
    "    target_features=water_main_fc,\n",
    "    join_features=lats_school_child_dissolve,\n",
    "    out_feature_class=Main_School_Child_Join,\n",
    "    join_operation=\"JOIN_ONE_TO_ONE\",\n",
    "    join_type=\"KEEP_COMMON\",\n",
    ")\n",
    "# turn the feature class into a dataframe\n",
    "main_school_child_df = pd.DataFrame.spatial.from_featureclass(Main_School_Child_Join)\n",
    "\n",
    "# drop all columns except OBJECTID, FACILITYID, zone, and PIN\n",
    "main_school_child_df = main_school_child_df[['OBJECTID', 'FACILITYID', 'FinalPIN']]\n",
    "# add a column called school_childcare and fill it with 1\n",
    "main_school_child_df['school_childcare'] = \"Connected\"\n",
    "main_school_child_df.head()\n",
    "# save the dataframes to csv file with directory path variable\n",
    "main_school_child_df.to_csv(os.path.join(dir_path, \"Results\", \"mains_school_childcare.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Medical Facilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='gpresult'><h2>Messages</h2><div id='messages'>Start Time: Monday, July 22, 2024 9:36:20 AM<br>Succeeded at Monday, July 22, 2024 9:36:20 AM (Elapsed Time: 0.16 seconds)<br></div><div id = 'default' /></div></div>"
      ],
      "text/plain": [
       "<Result 'in_memory\\\\medical_parcels'>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spatial join medical facilities to parcels\n",
    "parcels_fc = \"Parcels\"\n",
    "medical_facilities_fc = \"HealthCare\"\n",
    "\n",
    "# store output in memory\n",
    "medical_parcels = \"in_memory/medical_parcels\"\n",
    "\n",
    "arcpy.analysis.SpatialJoin(\n",
    "    target_features=parcels_fc,\n",
    "    join_features=medical_facilities_fc,\n",
    "    out_feature_class=medical_parcels,\n",
    "    join_operation=\"JOIN_ONE_TO_ONE\",\n",
    "    join_type=\"Keep_Common\",\n",
    ")\n",
    "# identify lateral lines that intersect the spatial join of parcels and medical facilities using spatial join\n",
    "lats_medical = \"in_memory/lats_medical\"\n",
    "\n",
    "arcpy.analysis.SpatialJoin(\n",
    "    target_features=lateral_lines_fc,\n",
    "    join_features=medical_parcels,\n",
    "    out_feature_class=lats_medical,\n",
    "    join_operation=\"JOIN_ONE_TO_ONE\",\n",
    "    join_type=\"KEEP_COMMON\",\n",
    ")\n",
    "# spatial join the medical lateral results to the rest of the laterals by intersect to identify both sides of the service that serve the medical facilities\n",
    "lats_medical_all = \"in_memory/lats_medical_all\"\n",
    "\n",
    "arcpy.analysis.SpatialJoin(\n",
    "    target_features=lateral_lines_fc,\n",
    "    join_features=lats_medical,\n",
    "    out_feature_class=lats_medical_all,\n",
    "    join_operation=\"JOIN_ONE_TO_ONE\",\n",
    "    join_type=\"KEEP_COMMON\",\n",
    "    match_option=\"INTERSECT\",\n",
    ")\n",
    "# dissolve the spatial join results by parcel id to create one line feature for each school/childcare service connection\n",
    "lats_medical_dissolve = \"in_memory/lats_medical_dissolve\"\n",
    "\n",
    "arcpy.management.Dissolve(\n",
    "    in_features=lats_medical_all,\n",
    "    out_feature_class=lats_medical_dissolve,\n",
    "    dissolve_field=\"FinalPIN\",\n",
    ")\n",
    "# spatial join the dissolved feature class to the water main feature class to identify the mains that serve the school/childcare\n",
    "main_medical = \"in_memory/main_medical\"\n",
    "\n",
    "arcpy.analysis.SpatialJoin(\n",
    "    target_features=water_main_fc,\n",
    "    join_features=lats_medical_dissolve,\n",
    "    out_feature_class=main_medical,\n",
    "    join_operation=\"JOIN_ONE_TO_ONE\",\n",
    "    join_type=\"KEEP_COMMON\",\n",
    ")\n",
    "# turn the feature class into a dataframe\n",
    "main_medical_df = pd.DataFrame.spatial.from_featureclass(main_medical)\n",
    "\n",
    "# drop all columns except OBJECTID, FACILITYID, and PARCEL_NO\n",
    "main_medical_df = main_medical_df[['OBJECTID', 'FACILITYID', 'FinalPIN']]\n",
    "# add a column called medical and fill it with 1\n",
    "main_medical_df['medical'] = \"Connected\"\n",
    "main_medical_df.head()\n",
    "# save the dataframes to csv file with directory path variable\n",
    "main_medical_df.to_csv(os.path.join(dir_path, \"Results\", \"mains_medical.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Critical Customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='gpresult'><h2>Messages</h2><div id='messages'>Start Time: Monday, July 22, 2024 9:36:21 AM<br>Succeeded at Monday, July 22, 2024 9:36:21 AM (Elapsed Time: 0.16 seconds)<br></div><div id = 'default' /></div></div>"
      ],
      "text/plain": [
       "<Result 'in_memory\\\\critical_cust_parcels'>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spatial join critical customer facilities to parcels\n",
    "parcels_fc = \"Parcels\"\n",
    "crit_customers_fc = \"AdditionalCriticalCustomers\"\n",
    "\n",
    "# store output in memory\n",
    "critical_cust_parcels = \"in_memory/critical_cust_parcels\"\n",
    "\n",
    "arcpy.analysis.SpatialJoin(\n",
    "    target_features=parcels_fc,\n",
    "    join_features=crit_customers_fc,\n",
    "    out_feature_class=critical_cust_parcels,\n",
    "    join_operation=\"JOIN_ONE_TO_ONE\",\n",
    "    join_type=\"Keep_Common\",\n",
    ")\n",
    "# identify lateral lines that intersect the spatial join of parcels and medical facilities using spatial join\n",
    "lats_critical_cust = \"in_memory/lats_critical_cust\"\n",
    "\n",
    "arcpy.analysis.SpatialJoin(\n",
    "    target_features=lateral_lines_fc,\n",
    "    join_features=critical_cust_parcels,\n",
    "    out_feature_class=lats_critical_cust,\n",
    "    join_operation=\"JOIN_ONE_TO_ONE\",\n",
    "    join_type=\"KEEP_COMMON\",\n",
    ")\n",
    "# spatial join the crit cust lateral results to the rest of the laterals by intersect to identify both sides of the service that serve the medical facilities\n",
    "lats_critical_cust_all = \"in_memory/lats_critical_cust_all\"\n",
    "\n",
    "arcpy.analysis.SpatialJoin(\n",
    "    target_features=lateral_lines_fc,\n",
    "    join_features=lats_critical_cust,\n",
    "    out_feature_class=lats_critical_cust_all,\n",
    "    join_operation=\"JOIN_ONE_TO_ONE\",\n",
    "    join_type=\"KEEP_COMMON\",\n",
    "    match_option=\"INTERSECT\",\n",
    ")\n",
    "# dissolve the spatial join results by parcel id to create one line feature for each school/childcare service connection\n",
    "lats_critical_cust_dissolve = \"in_memory/lats_critical_cust_dissolve\"\n",
    "\n",
    "arcpy.management.Dissolve(\n",
    "    in_features=lats_critical_cust_all,\n",
    "    out_feature_class=lats_critical_cust_dissolve,\n",
    "    dissolve_field=\"FinalPIN\",\n",
    ")\n",
    "# spatial join the dissolved feature class to the water main feature class to identify the mains that serve the school/childcare\n",
    "main_critical_cust = \"in_memory/main_critical_cust\"\n",
    "\n",
    "arcpy.analysis.SpatialJoin(\n",
    "    target_features=water_main_fc,\n",
    "    join_features=lats_critical_cust_dissolve,\n",
    "    out_feature_class=main_critical_cust,\n",
    "    join_operation=\"JOIN_ONE_TO_ONE\",\n",
    "    join_type=\"KEEP_COMMON\",\n",
    ")\n",
    "# turn the feature class into a dataframe\n",
    "main_critical_cust_df = pd.DataFrame.spatial.from_featureclass(main_critical_cust)\n",
    "\n",
    "# drop all columns except OBJECTID, FACILITYID, and PARCEL_NO\n",
    "main_critical_cust_df = main_critical_cust_df[['OBJECTID', 'FACILITYID', 'FinalPIN']]\n",
    "# add a column called medical and fill it with 1\n",
    "main_critical_cust_df['criticalcust'] = \"Connected\"\n",
    "main_critical_cust_df.head()\n",
    "# save the dataframes to csv file with directory path variable\n",
    "main_critical_cust_df.to_csv(os.path.join(dir_path, \"Results\", \"mains_critical_cust.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile data into one dataframe for calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile data from saved csv's in case calculations need to be redone based on just one updated parameter section\n",
    "\n",
    "1. Near_results.csv - distance to proximity features for each main segment\n",
    "2. mains_affected_lats_by_zones.csv - summary of isolation zones and affected customers per zone for each main segment\n",
    "3. mains_school_childcare.csv - mains connected to critical school/childcare customer\n",
    "4. mains_medical.csv - mains connected to critical medical facility customers\n",
    "5. mains_critical_cust.csv - mains connected to other critical customers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "dir_path = os.getcwd()\n",
    "# create dataframes from the csv files\n",
    "file_names = [\"NearResults.csv\", \"mains_affected_lats_by_zones.csv\", \"mains_school_childcare.csv\", \"mains_medical.csv\", \"mains_critical_cust.csv\"]\n",
    "\n",
    "# create each df individually\n",
    "near_df = pd.read_csv(os.path.join(dir_path, \"Results\", file_names[0]))\n",
    "connections_df = pd.read_csv(os.path.join(dir_path, \"Results\", file_names[1]))\n",
    "school_childcare_df = pd.read_csv(os.path.join(dir_path, \"Results\", file_names[2]))\n",
    "medical_df = pd.read_csv(os.path.join(dir_path, \"Results\", file_names[3]))\n",
    "critical_cust_df = pd.read_csv(os.path.join(dir_path, \"Results\", file_names[4]))\n",
    "near_df.head()\n",
    "# Get new water mains data frame after adding a field that calculates the length of the water mains\n",
    "# Set the workspace\n",
    "arcpy.env.workspace = r\"C:\\Users\\ggarcia\\OneDrive - Abonmarche\\GIS Projects\\2023\\23-0304 Grand Haven CDSMI\\ArcGIS Pro\\GH Risk Analysis\\Round3Analysis.gdb\"\n",
    "arcpy.env.overwriteOutput = True\n",
    "\n",
    "water_main_fc = \"Round3WaterMain\"\n",
    "\n",
    "# export the feature class to an in memory feature class\n",
    "water_main_fc_in_memory = \"in_memory\\water_main_fc\"\n",
    "\n",
    "arcpy.conversion.ExportFeatures(water_main_fc, water_main_fc_in_memory)\n",
    "\n",
    "arcpy.management.CalculateGeometryAttributes(\n",
    "    in_features=water_main_fc_in_memory,\n",
    "    geometry_property=[[\"LENGTH\", \"LENGTH\"]],\n",
    "    length_unit=\"FEET_INT\"\n",
    ")\n",
    "\n",
    "# convert the feature class to a dataframe\n",
    "water_main_df = pd.DataFrame.spatial.from_featureclass(water_main_fc_in_memory)\n",
    "# keep columns OBJECTID, FACILITYID, DIAMETER, and LENGTH\n",
    "water_main_df = water_main_df[['FACILITYID', 'DIAMETER', 'LENGTH']]\n",
    "water_main_df.head()\n",
    "# drop all rows with null values\n",
    "water_main_df.dropna(inplace=True)\n",
    "# convert the facilityid column in the near_df to a string then merge the near_df with the water_main_df by the FACILITYID column\n",
    "near_df['FACILITYID'] = near_df['FACILITYID'].astype(str)\n",
    "# remove the .0 from the FACILITYID column\n",
    "near_df['FACILITYID'] = near_df['FACILITYID'].str.split('.').str[0]\n",
    "\n",
    "# drop objectid and diameter from the near_df\n",
    "near_df = near_df.drop(columns=['OBJECTID', 'DIAMETER'])\n",
    "\n",
    "# merge the water_main_df and near_df with a join on FACILITYID from the water_main_df and FACILITYID from the near_df\n",
    "water_main_df['FACILITYID'] = water_main_df['FACILITYID'].astype(str)\n",
    "near_merged_df = pd.merge(water_main_df, near_df, on='FACILITYID', how='left')\n",
    "near_merged_df.head()\n",
    "# convert the facilityid column in the connections_df to a string then merge the connections_df with the water_main_df by the FACILITYID column\n",
    "connections_df['FACILITYID'] = connections_df['FACILITYID'].astype(str)\n",
    "# drop decimal and trailing numbers from the FACILITYID column\n",
    "connections_df['FACILITYID'] = connections_df['FACILITYID'].str.split('.').str[0]\n",
    "\n",
    "# drop objectid and diameter from the connections_df\n",
    "connections_df = connections_df.drop(columns=['OBJECTID', 'DIAMETER'])\n",
    "\n",
    "# merge the water_main_df and connections_df with a join on FACILITYID from the water_main_df and FACILITYID from the connections_df\n",
    "connections_near_merged_df = pd.merge(near_merged_df, connections_df, on='FACILITYID', how='left')\n",
    "connections_near_merged_df.head()\n",
    "# convert the facilityid column in the school_childcare_df to a string then merge the school_childcare_df with the water_main_df by the FACILITYID column\n",
    "school_childcare_df['FACILITYID'] = school_childcare_df['FACILITYID'].astype(str)\n",
    "# drop decimal and trailing numbers from the FACILITYID column\n",
    "school_childcare_df['FACILITYID'] = school_childcare_df['FACILITYID'].str.split('.').str[0]\n",
    "\n",
    "# drop objectid and diameter from the school_childcare_df\n",
    "school_childcare_df = school_childcare_df.drop(columns=['OBJECTID', 'FinalPIN'])\n",
    "\n",
    "# merge the water_main_df and school_childcare_df with a join on FACILITYID from the water_main_df and FACILITYID from the school_childcare_df\n",
    "school_connections_near_merged_df = pd.merge(connections_near_merged_df, school_childcare_df, on='FACILITYID', how='left')\n",
    "school_connections_near_merged_df.head()\n",
    "# convert the facilityid column in the medical_df to a string then merge the medical_df with the water_main_df by the FACILITYID column\n",
    "medical_df['FACILITYID'] = medical_df['FACILITYID'].astype(str)\n",
    "# drop decimal and trailing numbers from the FACILITYID column\n",
    "medical_df['FACILITYID'] = medical_df['FACILITYID'].str.split('.').str[0]\n",
    "\n",
    "# drop objectid and parcel_no from the medical_df\n",
    "medical_df = medical_df.drop(columns=['OBJECTID', 'FinalPIN'])\n",
    "\n",
    "# merge the water_main_df and medical_df with a join on FACILITYID from the water_main_df and FACILITYID from the medical_df\n",
    "final_df = pd.merge(school_connections_near_merged_df, medical_df, on='FACILITYID', how='left')\n",
    "final_df.head()\n",
    "# convert the facilityid column in the critical_cust_df to a string then merge the critical_cust_df with the water_main_df by the FACILITYID column\n",
    "critical_cust_df['FACILITYID'] = critical_cust_df['FACILITYID'].astype(str)\n",
    "# drop decimal and trailing numbers from the FACILITYID column\n",
    "critical_cust_df['FACILITYID'] = critical_cust_df['FACILITYID'].str.split('.').str[0]\n",
    "\n",
    "# drop objectid and parcel_no from the critical_cust_df\n",
    "critical_cust_df = critical_cust_df.drop(columns=['OBJECTID', 'FinalPIN'])\n",
    "\n",
    "# merge the water_main_df and critical_cust_df with a join on FACILITYID from the water_main_df and FACILITYID from the critical_cust_df\n",
    "final_df = pd.merge(final_df, critical_cust_df, on='FACILITYID', how='left')\n",
    "final_df.head()\n",
    "# save the final_df to a csv file using dir_path\n",
    "output_path = os.path.join(dir_path, \"Results\", \"Combined_Parameters.csv\")\n",
    "final_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolation Zone analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df from the csv file\n",
    "final_df = pd.read_csv(output_path)\n",
    "# for all school_childcare column values of connected store the zone value, if school_childcare is null and the zone is in the list of zones with connected school_childcare, set the value to zone\n",
    "zones = []\n",
    "\n",
    "# Iterate over the rows in the dataframe\n",
    "for index, row in final_df.iterrows():\n",
    "    # If the 'school_childcare' value is \"Connected\", add the zone to the list\n",
    "    if row['school_childcare'] == \"Connected\":\n",
    "        zones.append(row['zone'])\n",
    "\n",
    "# Convert the list to a set for faster lookup\n",
    "zones = set(zones)\n",
    "\n",
    "# Iterate over the rows in the dataframe again\n",
    "for index, row in final_df.iterrows():\n",
    "    # If the 'school_childcare' value is null and the zone is in the list, set the 'school_childcare' value to \"Zone\"\n",
    "    if pd.isnull(row['school_childcare']) and row['zone'] in zones:\n",
    "        final_df.loc[index, 'school_childcare'] = \"Zone\"\n",
    "\n",
    "# for all medical column values of connected store the zone value, if medical is null and the zone is in the list of zones with connected medical, set the value to zone\n",
    "zones = []\n",
    "\n",
    "# Iterate over the rows in the dataframe\n",
    "for index, row in final_df.iterrows():\n",
    "    # If the 'medical' value is \"Connected\", add the zone to the list\n",
    "    if row['medical'] == \"Connected\":\n",
    "        zones.append(row['zone'])\n",
    "\n",
    "# Convert the list to a set for faster lookup\n",
    "zones = set(zones)\n",
    "\n",
    "# Iterate over the rows in the dataframe again\n",
    "for index, row in final_df.iterrows():\n",
    "    # If the 'medical' value is null and the zone is in the list, set the 'medical' value to \"Zone\"\n",
    "    if pd.isnull(row['medical']) and row['zone'] in zones:\n",
    "        final_df.loc[index, 'medical'] = \"Zone\"\n",
    "# for all criticalcust column values of connected store the zone value, if criticalcust is null and the zone is in the list of zones with connected criticalcust, set the value to zone\n",
    "zones = []\n",
    "\n",
    "# Iterate over the rows in the dataframe\n",
    "for index, row in final_df.iterrows():\n",
    "    # If the 'criticalcust' value is \"Connected\", add the zone to the list\n",
    "    if row['criticalcust'] == \"Connected\":\n",
    "        zones.append(row['zone'])\n",
    "\n",
    "# Convert the list to a set for faster lookup\n",
    "zones = set(zones)\n",
    "\n",
    "# Iterate over the rows in the dataframe again\n",
    "for index, row in final_df.iterrows():\n",
    "    # If the 'criticalcust' value is null and the zone is in the list, set the 'criticalcust' value to \"Zone\"\n",
    "    if pd.isnull(row['criticalcust']) and row['zone'] in zones:\n",
    "        final_df.loc[index, 'criticalcust'] = \"Zone\"\n",
    "final_df.head()\n",
    "# save the final_df to a csv file using dir_path\n",
    "output_path = os.path.join(dir_path, \"Results\", \"Combined_Parameters_zone_check.csv\")\n",
    "final_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FACILITYID</th>\n",
       "      <th>DIAMETER</th>\n",
       "      <th>LENGTH</th>\n",
       "      <th>Buildings</th>\n",
       "      <th>Major_Intersection</th>\n",
       "      <th>Major_Road</th>\n",
       "      <th>Minor_Intersection</th>\n",
       "      <th>Minor_Road</th>\n",
       "      <th>Railroad</th>\n",
       "      <th>ROW</th>\n",
       "      <th>...</th>\n",
       "      <th>criticalcust</th>\n",
       "      <th>Railroad_score</th>\n",
       "      <th>WaterBodies_score</th>\n",
       "      <th>Buildings_score</th>\n",
       "      <th>Roadway_score</th>\n",
       "      <th>affected_lats_score</th>\n",
       "      <th>school_childcare_score</th>\n",
       "      <th>medical_score</th>\n",
       "      <th>critical_cust_score</th>\n",
       "      <th>DIAMETER_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>67.320030</td>\n",
       "      <td>43.911240</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>168.287292</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6489.103901</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>353.007987</td>\n",
       "      <td>80.060164</td>\n",
       "      <td>174.284811</td>\n",
       "      <td>195.279288</td>\n",
       "      <td>24.736512</td>\n",
       "      <td>1.345787</td>\n",
       "      <td>3080.661324</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>29.649311</td>\n",
       "      <td>43.100158</td>\n",
       "      <td>322.949458</td>\n",
       "      <td>299.657014</td>\n",
       "      <td>78.803958</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5462.586977</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>942.596550</td>\n",
       "      <td>32.930425</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>340.826979</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1722.953951</td>\n",
       "      <td>120.053457</td>\n",
       "      <td>939.162354</td>\n",
       "      <td>891.284866</td>\n",
       "      <td>623.562655</td>\n",
       "      <td>33.950529</td>\n",
       "      <td>4413.770733</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  FACILITYID  DIAMETER       LENGTH   Buildings  Major_Intersection  \\\n",
       "0          1       8.0    67.320030   43.911240            0.000000   \n",
       "1          2       6.0   353.007987   80.060164          174.284811   \n",
       "2          3       6.0    29.649311   43.100158          322.949458   \n",
       "3          4       4.0   942.596550   32.930425            0.000000   \n",
       "4          5      12.0  1722.953951  120.053457          939.162354   \n",
       "\n",
       "   Major_Road  Minor_Intersection  Minor_Road     Railroad  ROW  ...  \\\n",
       "0    0.000000          168.287292    0.000000  6489.103901  0.0  ...   \n",
       "1  195.279288           24.736512    1.345787  3080.661324  0.0  ...   \n",
       "2  299.657014           78.803958    0.000000  5462.586977  0.0  ...   \n",
       "3    0.000000            0.000000    0.000000   340.826979  0.0  ...   \n",
       "4  891.284866          623.562655   33.950529  4413.770733  0.0  ...   \n",
       "\n",
       "   criticalcust  Railroad_score  WaterBodies_score Buildings_score  \\\n",
       "0           NaN               0                  0               0   \n",
       "1           NaN               0                  0               0   \n",
       "2           NaN               0                  0               0   \n",
       "3           NaN               0                  0               0   \n",
       "4           NaN               0                  0               0   \n",
       "\n",
       "  Roadway_score affected_lats_score  school_childcare_score  medical_score  \\\n",
       "0             0                   0                       0              0   \n",
       "1             0                   0                       0              0   \n",
       "2             0                   0                       0              0   \n",
       "3             0                   0                       0              0   \n",
       "4             0                   0                       0              0   \n",
       "\n",
       "   critical_cust_score  DIAMETER_score  \n",
       "0                    0               0  \n",
       "1                    0               0  \n",
       "2                    0               0  \n",
       "3                    0               0  \n",
       "4                    0               0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop the zone column from the final_df\n",
    "final_df = final_df.drop(columns=['zone'])\n",
    "\n",
    "# add new columns with the specified names and fill them with 0\n",
    "final_df['Railroad_score'] = 0\n",
    "final_df['WaterBodies_score'] = 0\n",
    "final_df['Buildings_score'] = 0\n",
    "final_df['Roadway_score'] = 0\n",
    "final_df['affected_lats_score'] = 0\n",
    "final_df['school_childcare_score'] = 0\n",
    "final_df['medical_score'] = 0\n",
    "final_df['critical_cust_score'] = 0\n",
    "final_df['DIAMETER_score'] = 0\n",
    "\n",
    "final_df.head()\n",
    "def score_diameter(diameter):\n",
    "    if diameter < 4:\n",
    "        return 1\n",
    "    elif 4 <= diameter <= 8:\n",
    "        return 4\n",
    "    elif 8 < diameter <= 16:\n",
    "        return 7\n",
    "    elif diameter >= 16:\n",
    "        return 10\n",
    "\n",
    "# Apply the function to the 'DIAMETER' column to calculate the 'DIAMETER_score'\n",
    "final_df['DIAMETER_score'] = final_df['DIAMETER'].apply(score_diameter)\n",
    "final_df.head()\n",
    "def score_railroad(railroad):\n",
    "    if railroad == 0:\n",
    "        return 10\n",
    "    elif 0 < railroad <= 10:\n",
    "        return 9\n",
    "    elif 10 < railroad <= 50:\n",
    "        return 7\n",
    "    elif 50 < railroad <= 100:\n",
    "        return 5\n",
    "    elif railroad > 100:\n",
    "        return 0\n",
    "\n",
    "# Apply the function to the 'Railroad' column to calculate the 'Railroad_score'\n",
    "final_df['Railroad_score'] = final_df['Railroad'].apply(score_railroad)\n",
    "final_df.head()\n",
    "def score_waterbodies(WaterAreas, WaterLines):\n",
    "    waterbodies = min(WaterAreas, WaterLines)\n",
    "    if waterbodies == 0:\n",
    "        return 10\n",
    "    elif 0 < waterbodies <= 10:\n",
    "        return 9\n",
    "    elif 10 < waterbodies <= 50:\n",
    "        return 7\n",
    "    elif 50 < waterbodies <= 100:\n",
    "        return 5\n",
    "    elif waterbodies > 100:\n",
    "        return 0\n",
    "\n",
    "# Apply the function to the 'WaterAreas' and 'WaterLines' columns to calculate the 'WaterBodies_score'\n",
    "final_df['WaterBodies_score'] = final_df.apply(lambda row: score_waterbodies(row['WaterAreas'], row['WaterLines']), axis=1)\n",
    "final_df.head()\n",
    "def score_buildings(buildings):\n",
    "    if 0 <= buildings <= 5:\n",
    "        return 10\n",
    "    elif 5 < buildings <= 20:\n",
    "        return 5\n",
    "    elif buildings > 20:\n",
    "        return 0\n",
    "\n",
    "# Apply the function to the 'Buildings' column to calculate the 'Buildings_score'\n",
    "final_df['Buildings_score'] = final_df['Buildings'].apply(score_buildings)\n",
    "final_df.head()\n",
    "def score_affected_lats(affected_lats):\n",
    "    if pd.isnull(affected_lats) or affected_lats == 0:\n",
    "        return 0\n",
    "    elif affected_lats > 50:\n",
    "        return 10\n",
    "    elif 31 <= affected_lats <= 50:\n",
    "        return 8\n",
    "    elif 11 <= affected_lats <= 30:\n",
    "        return 5\n",
    "    elif 1 <= affected_lats <= 10:\n",
    "        return 1\n",
    "\n",
    "# Apply the function to the 'affected_lats' column to calculate the 'affected_lats_score'\n",
    "final_df['affected_lats_score'] = final_df['affected_lats'].apply(score_affected_lats)\n",
    "final_df.head()\n",
    "def score_school_childcare(school_childcare):\n",
    "    if school_childcare == \"Connected\":\n",
    "        return 10\n",
    "    elif school_childcare == \"Zone\":\n",
    "        return 8\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Apply the function to the 'school_childcare' column to calculate the 'school_childcare_score'\n",
    "final_df['school_childcare_score'] = final_df['school_childcare'].apply(score_school_childcare)\n",
    "final_df.head()\n",
    "def score_medical(medical):\n",
    "    if medical == \"Connected\":\n",
    "        return 10\n",
    "    elif medical == \"Zone\":\n",
    "        return 8\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Apply the function to the 'medical' column to calculate the 'medical_score'\n",
    "final_df['medical_score'] = final_df['medical'].apply(score_medical)\n",
    "final_df.head()\n",
    "def score_critical_cust(criticalcust):\n",
    "    if criticalcust == \"Connected\":\n",
    "        return 10\n",
    "    elif criticalcust == \"Zone\":\n",
    "        return 8\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "# Apply the function to the 'criticalcust' column to calculate the 'critical_cust_score'\n",
    "final_df['critical_cust_score'] = final_df['criticalcust'].apply(score_critical_cust)\n",
    "final_df.head()\n",
    "def score_roadway(row):\n",
    "    if row['Major_Intersection'] == 0:\n",
    "        return 10\n",
    "    elif row['Major_Road'] == 0:\n",
    "        return 9\n",
    "    elif row['Minor_Intersection'] == 0:\n",
    "        return 7\n",
    "    elif row['Minor_Road'] == 0:\n",
    "        return 6\n",
    "    elif row['ROW'] == 0 and row['Major_Road'] < row['Minor_Road']:\n",
    "        return 3\n",
    "    elif row['ROW'] == 0 and row['Major_Road'] >= row['Minor_Road']:\n",
    "        return 2\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Apply the function to the rows in the dataframe to calculate the 'Roadway_score'\n",
    "final_df['Roadway_score'] = final_df.apply(score_roadway, axis=1)\n",
    "final_df.head()\n",
    "# save the final_df to a csv file using dir_path\n",
    "output_path = os.path.join(dir_path, \"Results\", \"Scored_Parameters.csv\")\n",
    "final_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FACILITYID</th>\n",
       "      <th>DIAMETER</th>\n",
       "      <th>LENGTH</th>\n",
       "      <th>Buildings</th>\n",
       "      <th>Major_Intersection</th>\n",
       "      <th>Major_Road</th>\n",
       "      <th>Minor_Intersection</th>\n",
       "      <th>Minor_Road</th>\n",
       "      <th>Railroad</th>\n",
       "      <th>ROW</th>\n",
       "      <th>...</th>\n",
       "      <th>criticalcust</th>\n",
       "      <th>Railroad_score</th>\n",
       "      <th>WaterBodies_score</th>\n",
       "      <th>Buildings_score</th>\n",
       "      <th>Roadway_score</th>\n",
       "      <th>affected_lats_score</th>\n",
       "      <th>school_childcare_score</th>\n",
       "      <th>medical_score</th>\n",
       "      <th>critical_cust_score</th>\n",
       "      <th>DIAMETER_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>67.320030</td>\n",
       "      <td>43.911240</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>168.287292</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6489.103901</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>353.007987</td>\n",
       "      <td>80.060164</td>\n",
       "      <td>174.284811</td>\n",
       "      <td>195.279288</td>\n",
       "      <td>24.736512</td>\n",
       "      <td>1.345787</td>\n",
       "      <td>3080.661324</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>29.649311</td>\n",
       "      <td>43.100158</td>\n",
       "      <td>322.949458</td>\n",
       "      <td>299.657014</td>\n",
       "      <td>78.803958</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5462.586977</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>942.596550</td>\n",
       "      <td>32.930425</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>340.826979</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1722.953951</td>\n",
       "      <td>120.053457</td>\n",
       "      <td>939.162354</td>\n",
       "      <td>891.284866</td>\n",
       "      <td>623.562655</td>\n",
       "      <td>33.950529</td>\n",
       "      <td>4413.770733</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  FACILITYID  DIAMETER       LENGTH   Buildings  Major_Intersection  \\\n",
       "0          1       8.0    67.320030   43.911240            0.000000   \n",
       "1          2       6.0   353.007987   80.060164          174.284811   \n",
       "2          3       6.0    29.649311   43.100158          322.949458   \n",
       "3          4       4.0   942.596550   32.930425            0.000000   \n",
       "4          5      12.0  1722.953951  120.053457          939.162354   \n",
       "\n",
       "   Major_Road  Minor_Intersection  Minor_Road     Railroad  ROW  ...  \\\n",
       "0    0.000000          168.287292    0.000000  6489.103901  0.0  ...   \n",
       "1  195.279288           24.736512    1.345787  3080.661324  0.0  ...   \n",
       "2  299.657014           78.803958    0.000000  5462.586977  0.0  ...   \n",
       "3    0.000000            0.000000    0.000000   340.826979  0.0  ...   \n",
       "4  891.284866          623.562655   33.950529  4413.770733  0.0  ...   \n",
       "\n",
       "   criticalcust  Railroad_score  WaterBodies_score Buildings_score  \\\n",
       "0           NaN               0                  0               0   \n",
       "1           NaN               0                  0               0   \n",
       "2           NaN               0                  0               0   \n",
       "3           NaN               0                  0               0   \n",
       "4           NaN               0                  0               0   \n",
       "\n",
       "  Roadway_score affected_lats_score  school_childcare_score  medical_score  \\\n",
       "0            10                   5                       0              0   \n",
       "1             2                   1                       0              0   \n",
       "2             6                   0                       8              8   \n",
       "3            10                   5                       0              0   \n",
       "4             2                   1                       0              0   \n",
       "\n",
       "   critical_cust_score  DIAMETER_score  \n",
       "0                    0               4  \n",
       "1                    0               4  \n",
       "2                    0               4  \n",
       "3                    0               4  \n",
       "4                    0               7  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataframe from csv in case we want to adjust the calculation of scores we already created and add a COF column\n",
    "final_df = pd.read_csv(output_path)\n",
    "final_df.head()\n",
    "# remove any rows where length is null\n",
    "final_df = final_df[final_df['LENGTH'].notna()]\n",
    "# calculate the final COF and use math.ceil to round UP to the next whole number\n",
    "\n",
    "def calculate_final_score(row):\n",
    "    return math.ceil((row['DIAMETER_score'] * 0.1 +\n",
    "                  row['Railroad_score'] * 0.1 +\n",
    "                  row['Roadway_score'] * 0.15 +\n",
    "                  row['Buildings_score'] * 0.1 +\n",
    "                  row['affected_lats_score'] * 0.2 +\n",
    "                  row['WaterBodies_score'] * 0.2 +\n",
    "                  row['medical_score'] * 0.05 +\n",
    "                  row['school_childcare_score'] * 0.05 +\n",
    "                  row['critical_cust_score'] * 0.05))\n",
    "\n",
    "# Apply the function to the rows in the dataframe to calculate the 'final_score'\n",
    "final_df['COF'] = final_df.apply(calculate_final_score, axis=1)\n",
    "final_df.head()\n",
    "# add a column to normalize the COF to a 1-10 scale with a formula of (10/max cof) * cof\n",
    "final_df['COF_normalized'] = (10 / final_df['COF'].max()) * final_df['COF']\n",
    "# round up to the ceiling\n",
    "final_df['COF_normalized'] = final_df['COF_normalized'].apply(math.ceil)\n",
    "final_df.head()\n",
    "# Min-Max normalization to scale between 1 and 10\n",
    "min_val = final_df['COF'].min()\n",
    "max_val = final_df['COF'].max()\n",
    "final_df['COF_normalized'] = ((final_df['COF'] - min_val) / (max_val - min_val) * 9) + 1\n",
    "# round up to the ceiling\n",
    "final_df['COF_normalized'] = final_df['COF_normalized'].apply(math.ceil)\n",
    "final_df.head()\n",
    "# add column for adjusted_COF and fill it with the value of COF\n",
    "final_df['adjusted_COF'] = final_df['COF_normalized']\n",
    "\n",
    "# read the water_adjustments.csv file into a dataframe, use the FACILITYID column to change matching FACILITYID rows in the final_df and change the adjusted_COF to 10\n",
    "water_adjustments = pd.read_csv(os.path.join(dir_path, \"Results\", \"water_cof_adjustments.csv\"))\n",
    "\n",
    "# iterate over the rows in the water_adjustments dataframe\n",
    "for index, row in water_adjustments.iterrows():\n",
    "    # set the adjusted_COF to 10 for the rows in the final_df that match the FACILITYID in the water_adjustments dataframe\n",
    "    final_df.loc[final_df['FACILITYID'] == row['FACILITYID'], 'adjusted_COF'] = 10\n",
    "final_df.head()\n",
    "# save the final_df to a csv file using dir_path\n",
    "output_path = os.path.join(dir_path, \"Results\", \"Final_COF.csv\")\n",
    "final_df.to_csv(output_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
